{
    "dinov2-vit-large-p14": {
        "model_name": "dinov2-vit-large-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "dinov2-vit-large-p14_gLocal": {
        "model_name": "dinov2-vit-large-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": "gLocal"
    },
    "dino-vit-base-p16": {
        "model_name": "dino-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "dino-vit-base-p16_gLocal": {
        "model_name": "dino-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal"
    },
    "DreamSim_open_clip_vitb32": {
        "model_name": "DreamSim",
        "source": "custom",
        "model_parameters": {
            "variant": "open_clip_vitb32"
        },
        "module_name": "model.mlp",
        "objective": "Image-Text",
        "dataset": "LAION400M + NIGHTS",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null
    },
    "DreamSim_dino_vitb16": {
        "model_name": "DreamSim",
        "source": "custom",
        "model_parameters": {
            "variant": "dino_vitb16",
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "model.mlp",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k + NIGHTS",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "mae-vit-base-p16": {
        "model_name": "mae-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "mae-vit-large-p16": {
        "model_name": "mae-vit-large-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "mae-vit-huge-p14": {
        "model_name": "mae-vit-huge-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null
    },
    "OpenCLIP_RN50_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "RN50",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Transformer",
        "architecture": "ResNet",
        "embedding_dim": 1024,
        "alignment": null
    },
    "OpenCLIP_ViT-B-16_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null
    },
    "OpenCLIP_ViT-B-16_laion400m_e32": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null
    },
    "OpenCLIP_ViT-B-16_laion2b_s34b_b88k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "laion2b_s34b_b88k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null
    },
    "OpenCLIP_ViT-L-14_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "OpenCLIP_ViT-L-14_laion400m_e32": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "OpenCLIP_ViT-L-14_laion400m_e32_gLocal": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal"
    },
    "OpenCLIP_ViT-L-14_laion2b_s32b_b82k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion2b_s32b_b82k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "OpenCLIP_ViT-L-14_laion2b_s32b_b82k_gLocal": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion2b_s32b_b82k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal"
    },
    "OpenCLIP_ViT-B-16-SigLIP_webli": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16-SigLIP",
            "dataset": "webli"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WebLI",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "OpenCLIP_EVA01-g-14_laion400m_s11b_b41k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA01-g-14",
            "dataset": "laion400m_s11b_b41k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "OpenCLIP_EVA01-g-14-plus_merged2b_s11b_b114k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA01-g-14-plus",
            "dataset": "merged2b_s11b_b114k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "OpenCLIP_EVA02-L-14_merged2b_s4b_b131k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA02-L-14",
            "dataset": "merged2b_s4b_b131k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "OpenCLIP_EVA02-B-16_merged2b_s8b_b131k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA02-B-16",
            "dataset": "merged2b_s8b_b131k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null
    },
    "vgg16": {
        "model_name": "vgg16",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": null
    },
    "vgg16_gLocal": {
        "model_name": "vgg16",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": "gLocal"
    },
    "vgg19": {
        "model_name": "vgg19",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": null
    },
    "resnet50": {
        "model_name": "resnet50",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "resnet50_gLocal": {
        "model_name": "resnet50",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": "gLocal"
    },
    "resnet152": {
        "model_name": "resnet152",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "resnext50_32x4d": {
        "model_name": "resnext50_32x4d",
        "source": "timm",
        "model_parameters": {},
        "module_name": "global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNeXt",
        "embedding_dim": 2048,
        "alignment": null
    },
    "seresnet50": {
        "model_name": "seresnet50",
        "source": "timm",
        "model_parameters": {},
        "module_name": "global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "SE-ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "convnext_base": {
        "model_name": "convnext_base",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.flatten",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ConvNeXt",
        "embedding_dim": 1024,
        "alignment": null
    },
    "convnext_large": {
        "model_name": "convnext_large",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.flatten",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ConvNeXt",
        "embedding_dim": 1536,
        "alignment": null
    },
    "efficientnet_b3": {
        "model_name": "efficientnet_b3",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 1536,
        "alignment": null
    },
    "efficientnet_b4": {
        "model_name": "efficientnet_b4",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 1792,
        "alignment": null
    },
    "efficientnet_b5": {
        "model_name": "efficientnet_b5",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "efficientnet_b6": {
        "model_name": "efficientnet_b6",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2304,
        "alignment": null
    },
    "efficientnet_b7": {
        "model_name": "efficientnet_b7",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2560,
        "alignment": null
    },
    "simclr-rn50": {
        "model_name": "simclr-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "mocov2-rn50": {
        "model_name": "mocov2-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "vicreg-rn50": {
        "model_name": "vicreg-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "barlowtwins-rn50": {
        "model_name": "barlowtwins-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null
    },
    "vit_base_patch16_224": {
        "model_name": "vit_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "vit_base_patch16_224.augreg_in21k": {
        "model_name": "vit_base_patch16_224.augreg_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "vit_large_patch16_224": {
        "model_name": "vit_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "vit_large_patch16_224.augreg_in21k": {
        "model_name": "vit_large_patch16_224.augreg_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "vit_large_patch14_clip_224.laion2b": {
        "model_name": "vit_large_patch14_clip_224.laion2b",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "vit_huge_patch14_224.orig_in21k": {
        "model_name": "vit_huge_patch14_224.orig_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null
    },
    "vit_huge_patch14_clip_224.laion2b": {
        "model_name": "vit_huge_patch14_clip_224.laion2b",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null
    },
    "beit_base_patch16_224": {
        "model_name": "beit_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "beit_base_patch16_224.in22k_ft_in22k": {
        "model_name": "beit_base_patch16_224.in22k_ft_in22k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "beit_large_patch16_224": {
        "model_name": "beit_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "beit_large_patch16_224.in22k_ft_in22k": {
        "model_name": "beit_large_patch16_224.in22k_ft_in22k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "deit3_base_patch16_224": {
        "model_name": "deit3_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "deit3_base_patch16_224.fb_in22k_ft_in1k": {
        "model_name": "deit3_base_patch16_224.fb_in22k_ft_in1k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null
    },
    "deit3_large_patch16_224": {
        "model_name": "deit3_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "deit3_large_patch16_224.fb_in22k_ft_in1k": {
        "model_name": "deit3_large_patch16_224.fb_in22k_ft_in1k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null
    },
    "swin_base_patch4_window7_224": {
        "model_name": "swin_base_patch4_window7_224",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1024,
        "alignment": null
    },
    "swin_base_patch4_window7_224.ms_in22k": {
        "model_name": "swin_base_patch4_window7_224.ms_in22k",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1024,
        "alignment": null
    },
    "swin_large_patch4_window7_224": {
        "model_name": "swin_large_patch4_window7_224",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1536,
        "alignment": null
    },
    "swin_large_patch4_window7_224.ms_in22k": {
        "model_name": "swin_large_patch4_window7_224.ms_in22k",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1536,
        "alignment": null
    }
}