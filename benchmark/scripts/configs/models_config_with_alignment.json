{
    "dinov2-vit-large-p14": {
        "model_name": "dinov2-vit-large-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 304368640,
        "size_fmt": "304.4M",
        "size_class": "large"
    },
    "dinov2-vit-large-p14_gLocal": {
        "model_name": "dinov2-vit-large-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": "gLocal",
        "dataset_class": "Large DS",
        "size": 304368640,
        "size_fmt": "304.4M",
        "size_class": "large"
    },
    "dino-vit-base-p16": {
        "model_name": "dino-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 85798656,
        "size_fmt": "85.8M",
        "size_class": "small"
    },
    "dino-vit-base-p16_gLocal": {
        "model_name": "dino-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal",
        "dataset_class": "ImageNet1k",
        "size": 85798656,
        "size_fmt": "85.8M",
        "size_class": "small"
    },
    "DreamSim_open_clip_vitb32": {
        "model_name": "DreamSim",
        "source": "custom",
        "model_parameters": {
            "variant": "open_clip_vitb32"
        },
        "module_name": "model.mlp",
        "objective": "Image-Text",
        "dataset": "LAION400M + NIGHTS",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 88046592,
        "size_fmt": "88.0M",
        "size_class": "small"
    },
    "DreamSim_dino_vitb16": {
        "model_name": "DreamSim",
        "source": "custom",
        "model_parameters": {
            "variant": "dino_vitb16",
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "model.mlp",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k + NIGHTS",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 0,
        "size_fmt": "0",
        "size_class": "small"
    },
    "mae-vit-base-p16": {
        "model_name": "mae-vit-base-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 86388480,
        "size_fmt": "86.4M",
        "size_class": "small"
    },
    "mae-vit-large-p16": {
        "model_name": "mae-vit-large-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 303301632,
        "size_fmt": "303.3M",
        "size_class": "large"
    },
    "mae-vit-huge-p14": {
        "model_name": "mae-vit-huge-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 630764800,
        "size_fmt": "630.8M",
        "size_class": "xlarge"
    },
    "OpenCLIP_RN50_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "RN50",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 102007137,
        "size_fmt": "102.0M",
        "size_class": "medium"
    },
    "OpenCLIP_ViT-B-16_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 149620737,
        "size_fmt": "149.6M",
        "size_class": "medium"
    },
    "OpenCLIP_ViT-B-16_laion400m_e32": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 149620737,
        "size_fmt": "149.6M",
        "size_class": "medium"
    },
    "OpenCLIP_ViT-B-16_laion2b_s34b_b88k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16",
            "dataset": "laion2b_s34b_b88k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 149620737,
        "size_fmt": "149.6M",
        "size_class": "medium"
    },
    "OpenCLIP_ViT-L-14_openai": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "openai"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WIT-400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 427616513,
        "size_fmt": "427.6M",
        "size_class": "xlarge"
    },
    "OpenCLIP_ViT-L-14_laion400m_e32": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 427616513,
        "size_fmt": "427.6M",
        "size_class": "xlarge"
    },
    "OpenCLIP_ViT-L-14_laion400m_e32_gLocal": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion400m_e32"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal",
        "dataset_class": "Large DS",
        "size": 427616513,
        "size_fmt": "427.6M",
        "size_class": "xlarge"
    },
    "OpenCLIP_ViT-L-14_laion2b_s32b_b82k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion2b_s32b_b82k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 427616513,
        "size_fmt": "427.6M",
        "size_class": "xlarge"
    },
    "OpenCLIP_ViT-L-14_laion2b_s32b_b82k_gLocal": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-L-14",
            "dataset": "laion2b_s32b_b82k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": "gLocal",
        "dataset_class": "XLarge DS",
        "size": 427616513,
        "size_fmt": "427.6M",
        "size_class": "xlarge"
    },
    "OpenCLIP_ViT-B-16-SigLIP_webli": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "ViT-B-16-SigLIP",
            "dataset": "webli"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "WebLI",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 203155970,
        "size_fmt": "203.2M",
        "size_class": "medium"
    },
    "OpenCLIP_EVA01-g-14_laion400m_s11b_b41k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA01-g-14",
            "dataset": "laion400m_s11b_b41k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "LAION400M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 1136435841,
        "size_fmt": "1.1B",
        "size_class": "xlarge"
    },
    "OpenCLIP_EVA01-g-14-plus_merged2b_s11b_b114k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA01-g-14-plus",
            "dataset": "merged2b_s11b_b114k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 1366621569,
        "size_fmt": "1.4B",
        "size_class": "xlarge"
    },
    "OpenCLIP_EVA02-L-14_merged2b_s4b_b131k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA02-L-14",
            "dataset": "merged2b_s4b_b131k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 427755457,
        "size_fmt": "427.8M",
        "size_class": "xlarge"
    },
    "OpenCLIP_EVA02-B-16_merged2b_s8b_b131k": {
        "model_name": "OpenCLIP",
        "source": "custom",
        "model_parameters": {
            "variant": "EVA02-B-16",
            "dataset": "merged2b_s8b_b131k"
        },
        "module_name": "visual",
        "objective": "Image-Text",
        "dataset": "Merged2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 512,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 149691137,
        "size_fmt": "149.7M",
        "size_class": "medium"
    },
    "vgg16": {
        "model_name": "vgg16",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 138357544,
        "size_fmt": "138.4M",
        "size_class": "medium"
    },
    "vgg16_gLocal": {
        "model_name": "vgg16",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": "gLocal",
        "dataset_class": "ImageNet1k",
        "size": 138357544,
        "size_fmt": "138.4M",
        "size_class": "medium"
    },
    "vgg19": {
        "model_name": "vgg19",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "classifier.3",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "VGG",
        "embedding_dim": 4096,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 143667240,
        "size_fmt": "143.7M",
        "size_class": "medium"
    },
    "resnet50": {
        "model_name": "resnet50",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 25557032,
        "size_fmt": "25.6M",
        "size_class": "small"
    },
    "resnet50_gLocal": {
        "model_name": "resnet50",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": "gLocal",
        "dataset_class": "ImageNet1k",
        "size": 25557032,
        "size_fmt": "25.6M",
        "size_class": "small"
    },
    "resnet152": {
        "model_name": "resnet152",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 60192808,
        "size_fmt": "60.2M",
        "size_class": "small"
    },
    "resnext50_32x4d": {
        "model_name": "resnext50_32x4d",
        "source": "timm",
        "model_parameters": {},
        "module_name": "global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNeXt",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 25028904,
        "size_fmt": "25.0M",
        "size_class": "small"
    },
    "seresnet50": {
        "model_name": "seresnet50",
        "source": "timm",
        "model_parameters": {},
        "module_name": "global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "SE-ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 28088024,
        "size_fmt": "28.1M",
        "size_class": "small"
    },
    "convnext_base": {
        "model_name": "convnext_base",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.flatten",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ConvNeXt",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 88591464,
        "size_fmt": "88.6M",
        "size_class": "small"
    },
    "convnext_large": {
        "model_name": "convnext_large",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.flatten",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ConvNeXt",
        "embedding_dim": 1536,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 197767336,
        "size_fmt": "197.8M",
        "size_class": "medium"
    },
    "efficientnet_b3": {
        "model_name": "efficientnet_b3",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 1536,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 12233232,
        "size_fmt": "12.2M",
        "size_class": "small"
    },
    "efficientnet_b4": {
        "model_name": "efficientnet_b4",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 1792,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 19341616,
        "size_fmt": "19.3M",
        "size_class": "small"
    },
    "efficientnet_b5": {
        "model_name": "efficientnet_b5",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 30389784,
        "size_fmt": "30.4M",
        "size_class": "small"
    },
    "efficientnet_b6": {
        "model_name": "efficientnet_b6",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2304,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 43040704,
        "size_fmt": "43.0M",
        "size_class": "small"
    },
    "efficientnet_b7": {
        "model_name": "efficientnet_b7",
        "source": "torchvision",
        "model_parameters": {
            "weights": "DEFAULT"
        },
        "module_name": "avgpool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 2560,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 66347960,
        "size_fmt": "66.3M",
        "size_class": "small"
    },
    "simclr-rn50": {
        "model_name": "simclr-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "mocov2-rn50": {
        "model_name": "mocov2-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "vicreg-rn50": {
        "model_name": "vicreg-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "barlowtwins-rn50": {
        "model_name": "barlowtwins-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "embedding_dim": 2048,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "vit_base_patch16_224": {
        "model_name": "vit_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 86567656,
        "size_fmt": "86.6M",
        "size_class": "small"
    },
    "vit_base_patch16_224.augreg_in21k": {
        "model_name": "vit_base_patch16_224.augreg_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 102595923,
        "size_fmt": "102.6M",
        "size_class": "medium"
    },
    "vit_large_patch16_224": {
        "model_name": "vit_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 304326632,
        "size_fmt": "304.3M",
        "size_class": "large"
    },
    "vit_large_patch16_224.augreg_in21k": {
        "model_name": "vit_large_patch16_224.augreg_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 325690707,
        "size_fmt": "325.7M",
        "size_class": "large"
    },
    "vit_large_patch14_clip_224.laion2b": {
        "model_name": "vit_large_patch14_clip_224.laion2b",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 303966976,
        "size_fmt": "304.0M",
        "size_class": "large"
    },
    "vit_huge_patch14_224.orig_in21k": {
        "model_name": "vit_huge_patch14_224.orig_in21k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 630764800,
        "size_fmt": "630.8M",
        "size_class": "xlarge"
    },
    "vit_huge_patch14_clip_224.laion2b": {
        "model_name": "vit_huge_patch14_clip_224.laion2b",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Image-Text",
        "dataset": "LAION2B",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1280,
        "alignment": null,
        "dataset_class": "XLarge DS",
        "size": 632077824,
        "size_fmt": "632.1M",
        "size_class": "xlarge"
    },
    "beit_base_patch16_224": {
        "model_name": "beit_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 86530984,
        "size_fmt": "86.5M",
        "size_class": "small"
    },
    "beit_base_patch16_224.in22k_ft_in22k": {
        "model_name": "beit_base_patch16_224.in22k_ft_in22k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 102557713,
        "size_fmt": "102.6M",
        "size_class": "medium"
    },
    "beit_large_patch16_224": {
        "model_name": "beit_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 304430568,
        "size_fmt": "304.4M",
        "size_class": "large"
    },
    "beit_large_patch16_224.in22k_ft_in22k": {
        "model_name": "beit_large_patch16_224.in22k_ft_in22k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 325792593,
        "size_fmt": "325.8M",
        "size_class": "large"
    },
    "deit3_base_patch16_224": {
        "model_name": "deit3_base_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 86585320,
        "size_fmt": "86.6M",
        "size_class": "small"
    },
    "deit3_base_patch16_224.fb_in22k_ft_in1k": {
        "model_name": "deit3_base_patch16_224.fb_in22k_ft_in1k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 768,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 86585320,
        "size_fmt": "86.6M",
        "size_class": "small"
    },
    "deit3_large_patch16_224": {
        "model_name": "deit3_large_patch16_224",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 304374760,
        "size_fmt": "304.4M",
        "size_class": "large"
    },
    "deit3_large_patch16_224.fb_in22k_ft_in1k": {
        "model_name": "deit3_large_patch16_224.fb_in22k_ft_in1k",
        "source": "timm",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Supervised",
        "dataset": "ImageNet21k + ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet21k",
        "size": 304374760,
        "size_fmt": "304.4M",
        "size_class": "large"
    },
    "swin_base_patch4_window7_224": {
        "model_name": "swin_base_patch4_window7_224",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 87768224,
        "size_fmt": "87.8M",
        "size_class": "small"
    },
    "swin_base_patch4_window7_224.ms_in22k": {
        "model_name": "swin_base_patch4_window7_224.ms_in22k",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1024,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 109130249,
        "size_fmt": "109.1M",
        "size_class": "medium"
    },
    "swin_large_patch4_window7_224": {
        "model_name": "swin_large_patch4_window7_224",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1536,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 196532476,
        "size_fmt": "196.5M",
        "size_class": "medium"
    },
    "swin_large_patch4_window7_224.ms_in22k": {
        "model_name": "swin_large_patch4_window7_224.ms_in22k",
        "source": "timm",
        "model_parameters": {},
        "module_name": "head.global_pool",
        "objective": "Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "SwinTransformer",
        "embedding_dim": 1536,
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "size": 228565093,
        "size_fmt": "228.6M",
        "size_class": "medium"
    },
    "Kakaobrain_Align": {
        "model_name": "Kakaobrain_Align",
        "source": "custom",
        "model_parameters": {},
        "module_name": "pooler",
        "objective": "Image-Text",
        "dataset": "COYO-700M",
        "architecture_class": "Convolutional",
        "architecture": "EfficientNet",
        "embedding_dim": 640,
        "alignment": null,
        "dataset_class": "Large DS",
        "size": 62143440,
        "size_fmt": "62.1M",
        "size_class": "small"
    },
    "pirl-rn50": {
        "model_name": "pirl-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 2048,
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "jigsaw-rn50": {
        "model_name": "jigsaw-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 2048,
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "rotnet-rn50": {
        "model_name": "rotnet-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 2048,
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "swav-rn50": {
        "model_name": "swav-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 2048,
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "dino-rn50": {
        "model_name": "dino-rn50",
        "source": "ssl",
        "model_parameters": {},
        "module_name": "avgpool",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Convolutional",
        "architecture": "ResNet",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 2048,
        "size": 23508032,
        "size_fmt": "23.5M",
        "size_class": "small"
    },
    "dino-xcit-small-12-p16": {
        "model_name": "dino-xcit-small-12-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 384,
        "size": 25868304,
        "size_fmt": "25.9M",
        "size_class": "small"
    },
    "dino-xcit-medium-24-p16": {
        "model_name": "dino-xcit-medium-24-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 512,
        "size": 83882752,
        "size_fmt": "83.9M",
        "size_class": "small"
    },
    "dino-vit-small-p16": {
        "model_name": "dino-vit-small-p16",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "ImageNet1k",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "ImageNet1k",
        "embedding_dim": 384,
        "size": 21665664,
        "size_fmt": "21.7M",
        "size_class": "small"
    },
    "dinov2-vit-small-p14": {
        "model_name": "dinov2-vit-small-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "Large DS",
        "embedding_dim": 384,
        "size": 22056576,
        "size_fmt": "22.1M",
        "size_class": "small"
    },
    "dinov2-vit-base-p14": {
        "model_name": "dinov2-vit-base-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "Large DS",
        "embedding_dim": 768,
        "size": 86580480,
        "size_fmt": "86.6M",
        "size_class": "small"
    },
    "dinov2-vit-giant-p14": {
        "model_name": "dinov2-vit-giant-p14",
        "source": "ssl",
        "model_parameters": {
            "extract_cls_token": true,
            "token_extraction": "cls_token"
        },
        "module_name": "norm",
        "objective": "Self-Supervised",
        "dataset": "LVD-142M",
        "architecture_class": "Transformer",
        "architecture": "ViT",
        "alignment": null,
        "dataset_class": "Large DS",
        "embedding_dim": 1536,
        "size": 1136480768,
        "size_fmt": "1.1B",
        "size_class": "xlarge"
    }
}