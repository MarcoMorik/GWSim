{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook to check the sizes of the webdataset and get subsets if necessary\n",
    "In this notebook we load each dataset and check its size. If the size is larger than 10k samples, we create a subset of the dataset with a fixed number of samples per class. The subsets are stored in the folder `BASE_PATH_PROJECT/datasets/subsets` and are used for the experiments in the paper."
   ],
   "id": "fff1c7520ae623a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11e0a4-6a94-4918-903d-514e74c6d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from clip_benchmark.utils.utils import prepare_ds_name\n",
    "from constants import BASE_PATH_PROJECT\n",
    "\n",
    "sys.path.append('..')\n",
    "from scripts.helper import parse_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe46b4-fe52-41b3-a66c-8dfb507ac778",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"../scripts/webdatasets_wo_imagenet.txt\"\n",
    "\n",
    "features_base = BASE_PATH_PROJECT / 'features'\n",
    "dataset_base = BASE_PATH_PROJECT / 'datasets/subsets'\n",
    "\n",
    "base_model = 'dinov2-vit-large-p14'\n",
    "\n",
    "total_sample_nr = 10000\n",
    "\n",
    "np.random.seed(42)  ## IMPORTANT: SET SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbb3d7-1d6e-464b-ba03-fa0f4935ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = parse_datasets(datasets)\n",
    "datasets_features = [prepare_ds_name(ds) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0674dd-b40c-412d-9f61-1baa06c1f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_stats = {}\n",
    "for ds in datasets_features:\n",
    "    try:\n",
    "        df = pd.Series(torch.load(features_base / ds / base_model / 'targets_train.pt'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'No training data available for {ds=}')\n",
    "        continue\n",
    "    ds_stats[ds] = dict(\n",
    "        nsamples_train=len(df),\n",
    "        ncls_train=df.nunique(),\n",
    "        indices_dict={value: np.where(df == value)[0] for value in sorted(df.unique())},\n",
    "        indices_dict_les={value: len(np.where(df == value)[0]) for value in sorted(df.unique())}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea23c1-eb01-47cb-84fe-fa746917efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.DataFrame(ds_stats).T.sort_values('nsamples_train')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1d997-e81d-4ecd-b6ae-8a460cf166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.loc[datasets['nsamples_train'] > total_sample_nr, 'samples_per_class'] = np.ceil(\n",
    "    total_sample_nr / datasets['ncls_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99217c-16c5-4e91-97d5-907e3290296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(indices_dict, samples_per_class):\n",
    "    if np.isnan(samples_per_class):\n",
    "        return np.nan\n",
    "    subset_dict = {k: np.random.choice(v, min(len(v), samples_per_class), replace=False) for k, v in\n",
    "                   indices_dict.items()}\n",
    "    subset_dict = sorted(list(np.hstack(list(subset_dict.values()))))\n",
    "    return subset_dict\n",
    "\n",
    "\n",
    "datasets['subset_indices'] = datasets.apply(lambda x: create_subset(x['indices_dict'], x['samples_per_class']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628773e7-2cff-44e8-a2ea-98f01eb8bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, row in datasets.iterrows():\n",
    "    if row['nsamples_train'] > total_sample_nr:\n",
    "        storing_path = dataset_base / ds\n",
    "        storing_path.mkdir(parents=True, exist_ok=True)\n",
    "        with open(storing_path / 'subset_indices_train.json', 'w') as json_file:\n",
    "            tmp = [int(val) for val in row['subset_indices']]\n",
    "            json.dump(tmp, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
