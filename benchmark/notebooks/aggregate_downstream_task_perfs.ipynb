{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a540557eca33ce1b",
   "metadata": {},
   "source": [
    "## Notebook to aggregate the downstream task performances for each model\n",
    "After running single model evaluation with `../scripts/single_model_evaluation.py` for all models and datasets with can gather the linear probe results and store the aggregated information in a single csv file. "
   ]
  },
  {
   "cell_type": "code",
   "id": "60d4c5d5b774e88c",
   "metadata": {},
   "source": [
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from clip_benchmark.analysis.utils import retrieve_performance\n",
    "from constants import (\n",
    "    BASE_PATH_RESULTS,\n",
    "    ds_list_perf_file,\n",
    "    exclude_models,\n",
    "    model_config_file\n",
    ")\n",
    "from helper import (\n",
    "    load_all_datasetnames_n_info,\n",
    "    load_model_configs_and_allowed_models,\n",
    "    pp_storing_path\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d72ef81fa2ccaf6f",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "65d9c81e0c5f8cbe",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "ds_list_perf, ds_info = load_all_datasetnames_n_info(ds_list_perf_file, verbose=True)\n",
    "\n",
    "# Results of downstream task linear probes \n",
    "results_root = BASE_PATH_RESULTS / 'linear_probe/single_model'\n",
    "\n",
    "SAVE = False\n",
    "storing_path = pp_storing_path(BASE_PATH_RESULTS / f'aggregated/single_model_performance', SAVE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b868c7b9d33e597c",
   "metadata": {},
   "source": [
    "#### Load config"
   ]
  },
  {
   "cell_type": "code",
   "id": "ced88ccd0460de04",
   "metadata": {},
   "source": [
    "model_configs, allowed_models = load_model_configs_and_allowed_models(\n",
    "    path=model_config_file,\n",
    "    exclude_models=exclude_models,\n",
    "    exclude_alignment=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d284599b7172282b",
   "metadata": {},
   "source": [
    "### Aggreggate downstream task performance for all combinations of (ds_list_perf, allowed_models)"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "res = []\n",
    "for ds, mid in product(ds_list_perf, allowed_models):\n",
    "    performance = retrieve_performance(\n",
    "        model_id=mid,\n",
    "        dataset_id=ds,\n",
    "        metric_column='test_lp_acc1',\n",
    "        results_root=results_root,\n",
    "        regularization=\"weight_decay\",\n",
    "        allow_db_results=False\n",
    "    )\n",
    "    res.append({\n",
    "        'DS': ds,\n",
    "        'Model': mid,\n",
    "        'TestAcc': performance\n",
    "    })\n",
    "perf_res = pd.DataFrame(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cd5e93bc04c167b",
   "metadata": {},
   "source": [
    "if SAVE:\n",
    "    perf_res.to_csv(storing_path / f'all_ds.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
