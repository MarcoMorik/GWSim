{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Notebook 4.7: *Can representational similarity predict performance gaps?*",
   "id": "3de6ecfc4378f42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f88e1e-0ced-4d1e-8684-e5c6c3560294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from itertools import product\n",
    "import textwrap\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from constants import exclude_models, exclude_models_w_mae, cat_name_mapping, ds_info_file, model_config_file, fontsizes\n",
    "from helper import load_model_configs_and_allowed_models, save_or_show, load_ds_info\n",
    "\n",
    "sys.path.append('..')\n",
    "from scripts.helper import parse_datasets\n",
    "\n",
    "from clip_benchmark.analysis.utils import retrieve_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c7d53-9388-462f-aea4-1481454938fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_aggregated = Path('/home/space/diverse_priors/results/aggregated')\n",
    "\n",
    "### Config similarity data\n",
    "sim_data = pd.read_csv(base_path_aggregated / 'model_sims/all_metric_ds_model_pair_similarity.csv')\n",
    "\n",
    "### Config performance data\n",
    "ds_list_perf = parse_datasets('../scripts/webdatasets_w_in1k.txt')\n",
    "ds_list_perf = list(map(lambda x: x.replace('/', '_'), ds_list_perf))\n",
    "\n",
    "ds_info = load_ds_info(ds_info_file)\n",
    "\n",
    "results_root = '/home/space/diverse_priors/results/linear_probe/single_model'\n",
    "\n",
    "### Config datasets to include\n",
    "ds_to_include= set(ds_list_perf) - set(['cifar100-coarse', 'entity13']) \n",
    "ds_to_include.add('imagenet-subset-10k')\n",
    "remaining_ds = sorted(list(set(ds_list_perf) - set(ds_to_include)))\n",
    "\n",
    "## Storing information\n",
    "suffix = ''\n",
    "# suffix = '_ wo_mae'\n",
    "\n",
    "SAVE = True\n",
    "storing_path = Path(f'/home/space/diverse_priors/results/plots/scatter_sim_vs_performance_v2')\n",
    "if SAVE:\n",
    "    storing_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4109a6-f2e8-4357-94c6-9fdfda095f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter similarity data only for desired datasets\n",
    "print(sim_data.shape)\n",
    "if ds_to_include:\n",
    "    sim_data = sim_data[sim_data['DS'].isin(ds_to_include)].reset_index(drop=True)\n",
    "print(sim_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e4ea7207b2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename datasets with info\n",
    "sim_data['DS category'] = sim_data['DS'].apply(lambda x: ds_info.loc[x, 'domain'])\n",
    "sim_data['DS'] = sim_data['DS'].apply(lambda x: ds_info.loc[x, 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa022cd-d2a2-49e3-b32f-ba830e73caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-process 'pair' columns\n",
    "def pp_pair_col(df_col):\n",
    "    return df_col.apply(eval).apply(lambda x: f\"{cat_name_mapping[x[0]]}, {cat_name_mapping[x[1]]}\")\n",
    "\n",
    "\n",
    "pair_columns = [col for col in sim_data.columns if 'pair' in col]\n",
    "sim_data[pair_columns] = sim_data[pair_columns].apply(pp_pair_col, axis=0)\n",
    "pair_columns += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a906935-71d2-4ed4-955b-424e2f7c8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_excl_models = exclude_models_w_mae if 'mae' in suffix else exclude_models\n",
    "\n",
    "model_configs, allowed_models = load_model_configs_and_allowed_models(\n",
    "    path=model_config_file,\n",
    "    exclude_models=curr_excl_models,\n",
    "    exclude_alignment=True,\n",
    ")\n",
    "\n",
    "# allowed_models = sorted(list(set(allowed_models) - set(['jigsaw-rn50', 'rotnet-rn50'])))\n",
    "# len(allowed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb4868cec85324",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter only for allowed models\n",
    "sim_data = sim_data[sim_data['Model 1'].isin(allowed_models) & sim_data['Model 2'].isin(allowed_models)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae860b3f-bea0-4082-b244-ea22e73dd464",
   "metadata": {},
   "source": [
    "#### Retrieve the downstream task performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc003f-9ecd-417b-96fd-6ef8df216e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# # Ignore UserWarnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# res = []\n",
    "# for ds, mid in product(ds_list_perf, allowed_models):\n",
    "#     performance = retrieve_performance(\n",
    "#         model_id=mid, \n",
    "#         dataset_id=ds, \n",
    "#         metric_column='test_lp_acc1',\n",
    "#         results_root='/home/space/diverse_priors/results/linear_probe/single_model',\n",
    "#         regularization=\"weight_decay\",\n",
    "#         allow_db_results=False\n",
    "#     )\n",
    "#     res.append({\n",
    "#         'DS': ds,\n",
    "#         'Model': mid,\n",
    "#         'TestAcc': performance\n",
    "#     })\n",
    "# perf_res = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3e893-a2dd-46b0-9c7d-1dd195cf2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_res.to_csv(base_path_aggregated/ f'single_model_performance/all_ds{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea041adb-439f-41f2-bf7d-e6c6b7002ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_res = pd.read_csv(base_path_aggregated / f'single_model_performance/all_ds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442331298e1b7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_to_include:\n",
    "    perf_res = perf_res[perf_res['DS'].isin(ds_to_include)].reset_index(drop=True)\n",
    "perf_res['DS category'] = perf_res['DS'].apply(lambda x: ds_info.loc[x, 'domain'])\n",
    "perf_res['DS'] = perf_res['DS'].apply(lambda x: ds_info.loc[x, 'name'])\n",
    "perf_res = perf_res[perf_res['Model'].isin(allowed_models)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bc00b-d869-400e-9f01-bdbd7204125b",
   "metadata": {},
   "source": [
    "#### Combine model similarities and performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a588a9f-92df-4445-bcb1-bb8de121d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_perf(row):\n",
    "    m1_perf = perf_res.loc[(perf_res['Model'] == row['Model 1']) & (perf_res['DS'] == row['DS']), 'TestAcc'].item()\n",
    "    m2_perf = perf_res.loc[(perf_res['Model'] == row['Model 2']) & (perf_res['DS'] == row['DS']), 'TestAcc'].item()\n",
    "    return m1_perf, m2_perf, np.abs(m1_perf - m2_perf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fb4e6-5465-4286-9c86-b970694da0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_per_pair = pd.DataFrame(sim_data.apply(get_model_perf, axis=1).tolist(),\n",
    "                                    columns=['Model 1 perf.', 'Model 2 perf.', 'abs. diff. perf.']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05604c7-52f7-4cf1-87cc-51a10ab4fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_new = pd.concat([sim_data, performance_per_pair], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafaa1d-2b2d-41a0-8f2c-05d4db6c7016",
   "metadata": {},
   "source": [
    "#### Compute the correlations between the performance gaps and the model similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8822f47a78afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(subset_data):\n",
    "    corr_sp, _ = spearmanr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "    corr_pr, _ = pearsonr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "    return {'spearmanr': corr_sp, 'pearsonr': corr_pr}\n",
    "\n",
    "\n",
    "r_coeffs = sim_data_new.groupby(['Similarity metric', 'DS'])[['Similarity value', 'abs. diff. perf.']].apply(\n",
    "    get_correlation)\n",
    "r_coeffs = pd.DataFrame(r_coeffs.tolist(), index=r_coeffs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b95f3-c099-401d-a0e1-ba1bdbd7697b",
   "metadata": {},
   "source": [
    "#### Plot the performance vs. similarity scatter plots for 3 datasets per dataset category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28e0b141daf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_new = sim_data_new.sort_values(['DS category', 'DS']).reset_index(drop=True)\n",
    "sim_data_new['max_model_perf'] = sim_data_new[['Model 1 perf.', 'Model 2 perf.']].apply(max, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee4b307d39f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_order = ['ImageNet-1k', 'Flowers', 'Diabetic Retinopathy', 'DTD',\n",
    "            'CIFAR-100', 'Pets', 'EuroSAT', 'Dmlab',\n",
    "            'Entity-30', 'Stanford Cars', 'PCAM', 'FER2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d2c94-8a05-4775-ac06-33b277ff4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_consider = 'CKA linear'\n",
    "\n",
    "sim_data_new_subset = sim_data_new[sim_data_new['Similarity metric'] == metric_to_consider].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc737ecb-1021-45fd-82f7-8ac69c20f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_grid_v2(hue_col, figsize=(9, 7), corr_type='spearmanr'):\n",
    "    n, m = 3, 4\n",
    "    cm = 0.393701\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=m, figsize=(figsize[0] * cm * m, figsize[1] * cm * n), sharex=True,\n",
    "                             sharey=False)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (col, ax) in enumerate(zip(ds_order, axes)):\n",
    "        group_data = sim_data_new_subset[sim_data_new_subset['DS'] == col]\n",
    "        \n",
    "        # Create a norm for this subplot\n",
    "        vmin = group_data[hue_col].min()\n",
    "        vmax = group_data[hue_col].max()\n",
    "        norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        scatter = sns.scatterplot(\n",
    "            group_data,\n",
    "            x='Similarity value',\n",
    "            y='abs. diff. perf.',\n",
    "            hue=hue_col,\n",
    "            palette='viridis',\n",
    "            alpha=0.25,\n",
    "            ax=ax,\n",
    "            s=15,\n",
    "            legend=False,  # Don't show the legend\n",
    "            hue_norm=norm,  # Use the subplot-specific norm\n",
    "        )\n",
    "        \n",
    "        xlbl = 'Similarity value' if i // m == 2 else ''\n",
    "        ax.set_xlabel(xlbl, fontsize=fontsizes['label'])\n",
    "        ylbl = f'Model Performance Gap' if i % m == 0 else ''\n",
    "        ax.set_ylabel(ylbl, fontsize=fontsizes['label'])\n",
    "        col_cat = ds_info.loc[ds_info['name'] == col, 'domain'].unique()[0]\n",
    "        title = f\"$\\\\it{{{col_cat}}}$\\n{col}\" if i // m == 0 else col\n",
    "        ax.set_title(title, fontsize=fontsizes['title'])\n",
    "        \n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=fontsizes['ticks'])\n",
    "        \n",
    "        if corr_type == 'spearmanr':\n",
    "            r_coeff = r_coeffs.loc[(metric_to_consider, col), 'spearmanr']\n",
    "        else:\n",
    "            r_coeff = r_coeffs.loc[(metric_to_consider, col), 'pearsonr']\n",
    "        ax.text(0.9, 0.9, f'r coeff.: {r_coeff:.2f}',\n",
    "                transform=ax.transAxes, fontsize=fontsizes['label'],\n",
    "                bbox=dict(facecolor='white', alpha=0.5, edgecolor='white'),\n",
    "                ha='right')\n",
    "        \n",
    "        # Add a colorbar to each subplot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=fontsizes['ticks'])\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.4, hspace=0.3)  # Increase spacing to accommodate colorbars\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11618698-30c1-41d0-9450-e32c184766fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = get_scatter_grid_v2('max_model_perf', (9, 7), 'pearsonr')\n",
    "save_or_show(fig, storing_path / f'scatter_3_4_pearsonr_grid_cka_linear_max_model_perf.pdf', SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121253d-93c6-43bc-a4ad-71916bd184a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = get_scatter_grid_v2('max_model_perf', (9, 7), 'spearmanr')\n",
    "save_or_show(fig, storing_path / f'scatter_3_4_spearmanr_grid_cka_linear_max_model_perf.pdf', SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5659a3208dc1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_grid_v1(hue_col, figsize=(9, 7), corr_type='spearmanr'):\n",
    "    n, m = 3, 4\n",
    "    cm = 0.393701\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=m, figsize=(figsize[0] * cm * m, figsize[1] * cm * n), sharex=True,\n",
    "                             sharey=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (col, ax) in enumerate(zip(ds_order, axes)):\n",
    "        group_data = sim_data_new_subset[sim_data_new_subset['DS'] == col]\n",
    "        assert group_data['Similarity metric'].nunique() == 1\n",
    "\n",
    "        sns.scatterplot(\n",
    "            group_data,\n",
    "            x='Similarity value',\n",
    "            y='abs. diff. perf.',\n",
    "            hue=hue_col,\n",
    "            alpha=0.5,\n",
    "            ax=ax,\n",
    "            s=15,\n",
    "        )\n",
    "        xlbl = 'Similarity value' if i // m == 2 else ''\n",
    "        ax.set_xlabel(xlbl, fontsize=fontsizes['label'])\n",
    "\n",
    "        ylbl = f'Model Performance Gap' if i % m == 0 else ''\n",
    "        ax.set_ylabel(ylbl, fontsize=fontsizes['label'])\n",
    "\n",
    "        col_cat = ds_info.loc[ds_info['name'] == col, 'domain'].unique()[0]\n",
    "        # title = f\"{col_cat}\\n{col}\" if i//m == 0 else col\n",
    "        title = f\"$\\\\it{{{col_cat}}}$\\n{col}\" if i // m == 0 else col\n",
    "        ax.set_title(title, fontsize=fontsizes['title'])\n",
    "        \n",
    "        if i == 3 and hue_col:\n",
    "            sns.move_legend(ax,\n",
    "                            loc='upper left',\n",
    "                            title=hue_col,\n",
    "                            bbox_to_anchor=(1, 1), fontsize=fontsizes['legend'],\n",
    "                            title_fontsize=fontsizes['legend'], frameon=False)\n",
    "        elif hue_col:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.tick_params(axis='both',  # Apply to both x and y axes\n",
    "                       which='major',  # Apply to major ticks\n",
    "                       labelsize=fontsizes['ticks'])\n",
    "        if corr_type == 'spearmanr':\n",
    "            r_coeff = r_coeffs.loc[('CKA linear', col), 'spearmanr']\n",
    "        else:\n",
    "            r_coeff = r_coeffs.loc[('CKA linear', col), 'pearsonr']\n",
    "\n",
    "        ax.text(0.95, 0.9, f'r coeff.: {r_coeff:.2f}',\n",
    "                transform=ax.transAxes, fontsize=fontsizes['label'],\n",
    "                bbox=dict(facecolor='white', alpha=0.5),\n",
    "                ha='right')  # Align text to the right\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.1)\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb3bcb575c3f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "for corr_type in ['pearsonr', 'spearmanr']:\n",
    "    for hue_col, figsize in zip(pair_columns, [(9, 7), (9, 7), (10, 8), (10, 8), (8, 7), (8, 7)]):\n",
    "        fig = get_scatter_grid_v1(hue_col, figsize, corr_type)\n",
    "        if hue_col:\n",
    "            suffix = f'_{hue_col.replace(\" \", \"_\")}'\n",
    "        else:\n",
    "            suffix = \"\"\n",
    "        save_or_show(fig, storing_path / f'scatter_3_4_{corr_type}_grid_cka_linear{suffix}.pdf', SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef903e-dcdb-48c9-a0d4-e3add2d4f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imnet = sim_data_new_subset[sim_data_new_subset['DS'] == 'Flowers'].copy()\n",
    "# imnetsubset = imnet[['Model 1 perf.', 'Model 2 perf.', 'Similarity value']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb76b6-98c8-44d4-824b-4cc4b8b3fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scatter_to_grid_heatmap(ax, df, nbins=5):\n",
    "#     x_min, x_max = df['Model 1 perf.'].min(), df['Model 1 perf.'].max()\n",
    "#     y_min, y_max = df['Model 2 perf.'].min(), df['Model 2 perf.'].max()\n",
    "#     df['t1'] = pd.cut(df['Model 1 perf.'], bins=nbins, labels=range(nbins))\n",
    "#     df['t2'] = pd.cut(df['Model 2 perf.'], bins=nbins, labels=range(nbins))\n",
    "#     tbl = pd.pivot_table(\n",
    "#         df,\n",
    "#         values='Similarity value',\n",
    "#         index='t1',\n",
    "#         columns='t2',\n",
    "#         aggfunc='mean',\n",
    "#         observed=True\n",
    "#     )\n",
    "#     tbl = tbl.reindex(index=range(nbins), columns=range(nbins), fill_value=np.nan)\n",
    "    \n",
    "#     sns.heatmap(tbl, ax=ax, \n",
    "#                 vmin=0.2, vmax=0.95, \n",
    "#                 mask = tbl.isna())\n",
    "    \n",
    "#     ax.set_xticks(np.linspace(0, nbins, nbins+1))\n",
    "#     ax.set_yticks(np.linspace(0, nbins, nbins+1))\n",
    "#     ax.set_xticklabels([f'{x:.1f}' for x in np.linspace(x_min, x_max, nbins+1)])\n",
    "#     ax.set_yticklabels([f'{y:.1f}' for y in np.linspace(y_min, y_max, nbins+1)], rotation=90)\n",
    "    \n",
    "#     ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f0c6f-e9f9-40bc-8812-6e4925a0b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, m = 3, 4\n",
    "# cm = 0.393701\n",
    "# fig, axes = plt.subplots(nrows=n, ncols=m, \n",
    "#                          figsize=(10 * cm * m, 8 * cm * n), \n",
    "#                          sharex=False,\n",
    "#                          sharey=False)\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, (col, ax) in enumerate(zip(ds_order, axes)):\n",
    "\n",
    "#     group_data = sim_data_new_subset[sim_data_new_subset['DS'] == col]\n",
    "    \n",
    "#     scatter_to_grid_heatmap(ax, group_data.copy())\n",
    "#     ax.set_title(col)\n",
    "    \n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb05fe8c963a31",
   "metadata": {},
   "source": [
    "#### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2d00c-ad99-4e53-bad4-c963c6a72503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_scatter_n_compute_correlation(hue_col):\n",
    "#     n = sim_data_new['Similarity metric'].nunique()\n",
    "#     m = sim_data_new['DS'].nunique()\n",
    "#     box_size = 4\n",
    "# \n",
    "#     fig, axes = plt.subplots(nrows=n, ncols=m, figsize=(box_size * m, box_size * n), sharex=True, sharey=False)\n",
    "#     axes = axes.flatten()\n",
    "# \n",
    "#     r_vals_sr, r_vals_pr = [], []\n",
    "#     for i, (keys, group_data) in enumerate(sim_data_new.groupby(['Similarity metric', 'DS'])):\n",
    "#         ax = axes[i]\n",
    "#         sns.scatterplot(\n",
    "#             group_data,\n",
    "#             x='Similarity value',\n",
    "#             y='abs. diff. perf.',\n",
    "#             hue=hue_col,\n",
    "#             alpha=0.5,\n",
    "#             ax=ax,\n",
    "#             s=15,\n",
    "#         )\n",
    "# \n",
    "#         xlbl = 'Similarity value' if i // m == 1 else ''\n",
    "#         ax.set_xlabel(xlbl, fontsize=11)\n",
    "# \n",
    "#         ylbl = f'{keys[0]}\\n Model Performance Gap' if i % m == 0 else ''\n",
    "#         ax.set_ylabel(ylbl, fontsize=12)\n",
    "# \n",
    "#         title = keys[1] if i // m == 0 else ''\n",
    "#         ax.set_title(title, fontsize=12)\n",
    "# \n",
    "#         if i % m == (m - 1) and i // m == 0:\n",
    "#             sns.move_legend(ax,\n",
    "#                             loc='upper left',\n",
    "#                             title=hue_col,\n",
    "#                             bbox_to_anchor=(1, 1), fontsize=12,\n",
    "#                             title_fontsize=12, frameon=False)\n",
    "#         else:\n",
    "#             ax.get_legend().remove()\n",
    "# \n",
    "#         sp_r_coeff = r_coeffs.loc[keys, 'spearmanr']\n",
    "#         pr_r_coeff = r_coeffs.loc[keys, 'pearsonr']\n",
    "# \n",
    "#         ax.text(0.95, 0.9, f'r_spearman: {sp_r_coeff:.2f}',\n",
    "#                 transform=ax.transAxes, fontsize=11,\n",
    "#                 bbox=dict(facecolor='white', alpha=0.5),\n",
    "#                 ha='right')  # Align text to the right\n",
    "# \n",
    "#         ax.text(0.95, 0.8, f'r_pearson: {pr_r_coeff:.2f}',\n",
    "#                 transform=ax.transAxes, fontsize=11,\n",
    "#                 bbox=dict(facecolor='white', alpha=0.5),\n",
    "#                 ha='right')  # Align text to the right\n",
    "# \n",
    "#         # for cat, subset_data in group_data.groupby(hue_col):\n",
    "#         #     corr_sp, _ = spearmanr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "#         #     corr_pr, _ = pearsonr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "#         #     r_vals_sr.append(tuple(list(keys) + [cat, corr_sp]))\n",
    "#         #     r_vals_pr.append(tuple(list(keys) + [cat, corr_pr]))\n",
    "# \n",
    "#     fig.subplots_adjust(wspace=0.2, hspace=0.1)\n",
    "#     # r_vals_sr = pd.DataFrame(r_vals_sr, columns=['DS', 'Similarity metric', hue_col, 'corr'])\n",
    "#     # r_vals_pr = pd.DataFrame(r_vals_pr, columns=['DS', 'Similarity metric', hue_col, 'corr'])\n",
    "# \n",
    "#     return fig, r_vals_sr, r_vals_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821f002-2476-46c6-9a77-b431d3419d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style('ticks')\n",
    "# for hue_col in pair_columns:\n",
    "#     fig, _, _ = get_scatter_n_compute_correlation(hue_col)\n",
    "#     curr_suffix = suffix + f'_{\"_\".join(ds_to_include)}' if ds_to_include else suffix\n",
    "#     save_or_show(fig, storing_path / f'scatter_{hue_col.replace(\" \", \"_\")}{curr_suffix}.pdf', SAVE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
