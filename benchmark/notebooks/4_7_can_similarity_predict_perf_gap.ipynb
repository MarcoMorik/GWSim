{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook 4.7: *Can representational similarity predict performance gaps?*\n",
    "This notebook creates scatter plots of the performance gaps between models on a downstream task against their representational similarity. The representational similarity is computed using the CKA linear metric. The performance gaps are computed as the absolute difference in the performance of the two models on the downstream task. The notebook also computes the Spearman and Pearson r correlation between the performance gaps and the representational similarity. The notebook creates scatter plots for three datasets per dataset category."
   ],
   "id": "3de6ecfc4378f42"
  },
  {
   "cell_type": "code",
   "id": "22f88e1e-0ced-4d1e-8684-e5c6c3560294",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from constants import (exclude_models, exclude_models_w_mae, cat_name_mapping, model_config_file,\n",
    "                       fontsizes, BASE_PATH_RESULTS)\n",
    "from helper import (load_model_configs_and_allowed_models, save_or_show, load_all_datasetnames_n_info,\n",
    "                    pp_storing_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global variables",
   "id": "411503f551fd5e9c"
  },
  {
   "cell_type": "code",
   "id": "4a9c7d53-9388-462f-aea4-1481454938fb",
   "metadata": {},
   "source": [
    "### Config similarity data\n",
    "sim_data_path = BASE_PATH_RESULTS / 'aggregated' / 'model_sims/all_metric_ds_model_pair_similarity.csv'\n",
    "assert sim_data_path.exists(), f\"Path does not exist: {sim_data_path}. Aggregated similarity data not found, please run `aggregate_similarities_across_datasets.ipynb` before.\"\n",
    "\n",
    "### Config performance data\n",
    "perf_data_path = BASE_PATH_RESULTS / f'aggregated/single_model_performance/all_ds.csv'\n",
    "assert perf_data_path.exists(), f\"Path does not exist: {perf_data_path}. Aggregated performance data not found, please run `aggregate_downstream_task_perfs.ipynb` before.\"\n",
    "\n",
    "### Config performance data\n",
    "ds_list_perf, ds_info = load_all_datasetnames_n_info('../scripts/webdatasets_w_in1k.txt', verbose=True)\n",
    "\n",
    "### Config datasets to include\n",
    "ds_to_include = set(ds_list_perf) - set(['cifar100-coarse', 'entity13'])\n",
    "ds_to_include.add('imagenet-subset-10k')\n",
    "remaining_ds = sorted(list(set(ds_list_perf) - set(ds_to_include)))\n",
    "\n",
    "## Storing information\n",
    "suffix = ''\n",
    "# suffix = '_ wo_mae'\n",
    "\n",
    "SAVE = False\n",
    "storing_path = pp_storing_path(BASE_PATH_RESULTS / f'plots/scatter_sim_vs_performance_v2', SAVE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load model configurations and allowed models",
   "id": "873413271888f72e"
  },
  {
   "cell_type": "code",
   "id": "4a906935-71d2-4ed4-955b-424e2f7c8fb8",
   "metadata": {},
   "source": [
    "curr_excl_models = exclude_models_w_mae if 'mae' in suffix else exclude_models\n",
    "\n",
    "model_configs, allowed_models = load_model_configs_and_allowed_models(\n",
    "    path=model_config_file,\n",
    "    exclude_models=curr_excl_models,\n",
    "    exclude_alignment=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load similarity data",
   "id": "8dbf3fcb8260d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sim_data = pd.read_csv(sim_data_path)",
   "id": "bfc80e0abcfec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Filter similarity data only for desired datasets\n",
    "print(sim_data.shape)\n",
    "if ds_to_include:\n",
    "    sim_data = sim_data[sim_data['DS'].isin(ds_to_include)].reset_index(drop=True)\n",
    "print(sim_data.shape)"
   ],
   "id": "ce4109a6-f2e8-4357-94c6-9fdfda095f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Rename datasets with info\n",
    "sim_data['DS category'] = sim_data['DS'].apply(lambda x: ds_info.loc[x, 'domain'])\n",
    "sim_data['DS'] = sim_data['DS'].apply(lambda x: ds_info.loc[x, 'name'])"
   ],
   "id": "e86e4ea7207b2abc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Post-process 'pair' columns\n",
    "def pp_pair_col(df_col):\n",
    "    return df_col.apply(eval).apply(lambda x: f\"{cat_name_mapping[x[0]]}, {cat_name_mapping[x[1]]}\")\n",
    "\n",
    "\n",
    "pair_columns = [col for col in sim_data.columns if 'pair' in col]\n",
    "sim_data[pair_columns] = sim_data[pair_columns].apply(pp_pair_col, axis=0)\n",
    "pair_columns += [None]"
   ],
   "id": "faa022cd-d2a2-49e3-b32f-ba830e73caa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fbb4868cec85324",
   "metadata": {},
   "source": [
    "## Filter only for allowed models\n",
    "sim_data = sim_data[sim_data['Model 1'].isin(allowed_models) & sim_data['Model 2'].isin(allowed_models)].reset_index(\n",
    "    drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae860b3f-bea0-4082-b244-ea22e73dd464",
   "metadata": {},
   "source": "#### Load performance data"
  },
  {
   "cell_type": "code",
   "id": "ea041adb-439f-41f2-bf7d-e6c6b7002ab5",
   "metadata": {},
   "source": "perf_res = pd.read_csv(perf_data_path)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "442331298e1b7c85",
   "metadata": {},
   "source": [
    "if ds_to_include:\n",
    "    perf_res = perf_res[perf_res['DS'].isin(ds_to_include)].reset_index(drop=True)\n",
    "perf_res['DS category'] = perf_res['DS'].apply(lambda x: ds_info.loc[x, 'domain'])\n",
    "perf_res['DS'] = perf_res['DS'].apply(lambda x: ds_info.loc[x, 'name'])\n",
    "perf_res = perf_res[perf_res['Model'].isin(allowed_models)].reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "240bc00b-d869-400e-9f01-bdbd7204125b",
   "metadata": {},
   "source": [
    "#### Combine model similarities and performance measures"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a588a9f-92df-4445-bcb1-bb8de121d082",
   "metadata": {},
   "source": [
    "def get_model_perf(row):\n",
    "    m1_perf = perf_res.loc[(perf_res['Model'] == row['Model 1']) & (perf_res['DS'] == row['DS']), 'TestAcc'].item()\n",
    "    m2_perf = perf_res.loc[(perf_res['Model'] == row['Model 2']) & (perf_res['DS'] == row['DS']), 'TestAcc'].item()\n",
    "    return m1_perf, m2_perf, np.abs(m1_perf - m2_perf)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a23fb4e6-5465-4286-9c86-b970694da0b7",
   "metadata": {},
   "source": [
    "performance_per_pair = pd.DataFrame(sim_data.apply(get_model_perf, axis=1).tolist(),\n",
    "                                    columns=['Model 1 perf.', 'Model 2 perf.', 'abs. diff. perf.']).reset_index(\n",
    "    drop=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e05604c7-52f7-4cf1-87cc-51a10ab4fa08",
   "metadata": {},
   "source": [
    "sim_data_new = pd.concat([sim_data, performance_per_pair], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aeafaa1d-2b2d-41a0-8f2c-05d4db6c7016",
   "metadata": {},
   "source": [
    "#### Compute the correlations between the performance gaps and the model similarities"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9a8822f47a78afd",
   "metadata": {},
   "source": [
    "def get_correlation(subset_data):\n",
    "    corr_sp, _ = spearmanr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "    corr_pr, _ = pearsonr(subset_data['Similarity value'], subset_data['abs. diff. perf.'])\n",
    "    return {'spearmanr': corr_sp, 'pearsonr': corr_pr}\n",
    "\n",
    "\n",
    "r_coeffs = sim_data_new.groupby(['Similarity metric', 'DS'])[['Similarity value', 'abs. diff. perf.']].apply(\n",
    "    get_correlation)\n",
    "r_coeffs = pd.DataFrame(r_coeffs.tolist(), index=r_coeffs.index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "770b95f3-c099-401d-a0e1-ba1bdbd7697b",
   "metadata": {},
   "source": "### Plot the performance vs. similarity scatter plots for 3 datasets per dataset category"
  },
  {
   "cell_type": "code",
   "id": "2d28e0b141daf159",
   "metadata": {},
   "source": [
    "sim_data_new = sim_data_new.sort_values(['DS category', 'DS']).reset_index(drop=True)\n",
    "sim_data_new['max_model_perf'] = sim_data_new[['Model 1 perf.', 'Model 2 perf.']].apply(max, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8ee4b307d39f7f9",
   "metadata": {},
   "source": [
    "ds_order = ['ImageNet-1k', 'Flowers', 'Diabetic Retinopathy', 'DTD',\n",
    "            'CIFAR-100', 'Pets', 'EuroSAT', 'Dmlab',\n",
    "            'Entity-30', 'Stanford Cars', 'PCAM', 'FER2013']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "362d2c94-8a05-4775-ac06-33b277ff4211",
   "metadata": {},
   "source": [
    "metric_to_consider = 'CKA linear'\n",
    "\n",
    "sim_data_new_subset = sim_data_new[sim_data_new['Similarity metric'] == metric_to_consider].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc737ecb-1021-45fd-82f7-8ac69c20f633",
   "metadata": {},
   "source": [
    "def get_scatter_grid_v2(hue_col, figsize=(9, 7), corr_type='spearmanr'):\n",
    "    n, m = 3, 4\n",
    "    cm = 0.393701\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=m, figsize=(figsize[0] * cm * m, figsize[1] * cm * n), sharex=True,\n",
    "                             sharey=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (col, ax) in enumerate(zip(ds_order, axes)):\n",
    "        group_data = sim_data_new_subset[sim_data_new_subset['DS'] == col]\n",
    "\n",
    "        # Create a norm for this subplot\n",
    "        vmin = group_data[hue_col].min()\n",
    "        vmax = group_data[hue_col].max()\n",
    "        norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        scatter = sns.scatterplot(\n",
    "            group_data,\n",
    "            x='Similarity value',\n",
    "            y='abs. diff. perf.',\n",
    "            hue=hue_col,\n",
    "            palette='viridis',\n",
    "            alpha=0.25,\n",
    "            ax=ax,\n",
    "            s=15,\n",
    "            legend=False,  # Don't show the legend\n",
    "            hue_norm=norm,  # Use the subplot-specific norm\n",
    "        )\n",
    "\n",
    "        xlbl = 'Similarity value' if i // m == 2 else ''\n",
    "        ax.set_xlabel(xlbl, fontsize=fontsizes['label'])\n",
    "        ylbl = f'Model Performance Gap' if i % m == 0 else ''\n",
    "        ax.set_ylabel(ylbl, fontsize=fontsizes['label'])\n",
    "        col_cat = ds_info.loc[ds_info['name'] == col, 'domain'].unique()[0]\n",
    "        title = f\"$\\\\it{{{col_cat}}}$\\n{col}\" if i // m == 0 else col\n",
    "        ax.set_title(title, fontsize=fontsizes['title'])\n",
    "\n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=fontsizes['ticks'])\n",
    "\n",
    "        if corr_type == 'spearmanr':\n",
    "            r_coeff = r_coeffs.loc[(metric_to_consider, col), 'spearmanr']\n",
    "        else:\n",
    "            r_coeff = r_coeffs.loc[(metric_to_consider, col), 'pearsonr']\n",
    "        ax.text(0.9, 0.9, f'r coeff.: {r_coeff:.2f}',\n",
    "                transform=ax.transAxes, fontsize=fontsizes['label'],\n",
    "                bbox=dict(facecolor='white', alpha=0.5, edgecolor='white'),\n",
    "                ha='right')\n",
    "\n",
    "        # Add a colorbar to each subplot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, cax=cax)\n",
    "        cbar.ax.tick_params(labelsize=fontsizes['ticks'])\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.4, hspace=0.3)  # Increase spacing to accommodate colorbars\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot the scatter plots colored by the maximum performance of the two models",
   "id": "c780817dd8613af2"
  },
  {
   "cell_type": "code",
   "id": "11618698-30c1-41d0-9450-e32c184766fa",
   "metadata": {},
   "source": [
    "fig = get_scatter_grid_v2('max_model_perf', (9, 7), 'pearsonr')\n",
    "save_or_show(fig, storing_path / f'scatter_3_4_pearsonr_grid_cka_linear_max_model_perf.pdf', SAVE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = get_scatter_grid_v2('max_model_perf', (9, 7), 'spearmanr')\n",
    "save_or_show(fig, storing_path / f'scatter_3_4_spearmanr_grid_cka_linear_max_model_perf.pdf', SAVE)"
   ],
   "id": "91c685cafc2c0ddd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot the scatter plots with all points colored the same or by combination of model categories",
   "id": "9cccef22942fe83"
  },
  {
   "cell_type": "code",
   "id": "b5659a3208dc1f59",
   "metadata": {},
   "source": [
    "def get_scatter_grid_v1(hue_col, figsize=(9, 7), corr_type='spearmanr'):\n",
    "    n, m = 3, 4\n",
    "    cm = 0.393701\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=m, figsize=(figsize[0] * cm * m, figsize[1] * cm * n), sharex=True,\n",
    "                             sharey=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (col, ax) in enumerate(zip(ds_order, axes)):\n",
    "        group_data = sim_data_new_subset[sim_data_new_subset['DS'] == col]\n",
    "        assert group_data['Similarity metric'].nunique() == 1\n",
    "\n",
    "        sns.scatterplot(\n",
    "            group_data,\n",
    "            x='Similarity value',\n",
    "            y='abs. diff. perf.',\n",
    "            hue=hue_col,\n",
    "            alpha=0.5,\n",
    "            ax=ax,\n",
    "            s=15,\n",
    "        )\n",
    "        xlbl = 'Similarity value' if i // m == 2 else ''\n",
    "        ax.set_xlabel(xlbl, fontsize=fontsizes['label'])\n",
    "\n",
    "        ylbl = f'Model Performance Gap' if i % m == 0 else ''\n",
    "        ax.set_ylabel(ylbl, fontsize=fontsizes['label'])\n",
    "\n",
    "        col_cat = ds_info.loc[ds_info['name'] == col, 'domain'].unique()[0]\n",
    "        # title = f\"{col_cat}\\n{col}\" if i//m == 0 else col\n",
    "        title = f\"$\\\\it{{{col_cat}}}$\\n{col}\" if i // m == 0 else col\n",
    "        ax.set_title(title, fontsize=fontsizes['title'])\n",
    "\n",
    "        if i == 3 and hue_col:\n",
    "            sns.move_legend(ax,\n",
    "                            loc='upper left',\n",
    "                            title=hue_col,\n",
    "                            bbox_to_anchor=(1, 1), fontsize=fontsizes['legend'],\n",
    "                            title_fontsize=fontsizes['legend'], frameon=False)\n",
    "        elif hue_col:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "        ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.1f'))\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "        ax.tick_params(axis='both',  # Apply to both x and y axes\n",
    "                       which='major',  # Apply to major ticks\n",
    "                       labelsize=fontsizes['ticks'])\n",
    "        if corr_type == 'spearmanr':\n",
    "            r_coeff = r_coeffs.loc[('CKA linear', col), 'spearmanr']\n",
    "        else:\n",
    "            r_coeff = r_coeffs.loc[('CKA linear', col), 'pearsonr']\n",
    "\n",
    "        ax.text(0.95, 0.9, f'r coeff.: {r_coeff:.2f}',\n",
    "                transform=ax.transAxes, fontsize=fontsizes['label'],\n",
    "                bbox=dict(facecolor='white', alpha=0.5),\n",
    "                ha='right')  # Align text to the right\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0.1)\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4bfb3bcb575c3f1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sns.set_style('ticks')\n",
    "for corr_type in ['pearsonr', 'spearmanr']:\n",
    "    for hue_col, figsize in zip(pair_columns, [(9, 7), (9, 7), (10, 8), (10, 8), (8, 7), (8, 7)]):\n",
    "        fig = get_scatter_grid_v1(hue_col, figsize, corr_type)\n",
    "        if hue_col:\n",
    "            suffix = f'_{hue_col.replace(\" \", \"_\")}'\n",
    "        else:\n",
    "            suffix = \"\"\n",
    "        save_or_show(fig, storing_path / f'scatter_3_4_{corr_type}_grid_cka_linear{suffix}.pdf', SAVE)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
